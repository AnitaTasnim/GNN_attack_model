{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6Fz2wMoA1RNn5advOcPzx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnitaTasnim/GNN_attack_model/blob/main/nnn_drAlom_provided.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hoynn8xeuJI"
      },
      "outputs": [],
      "source": [
        "\n",
        "import itertools\n",
        "import numpy\n",
        "import networkx as nx\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy.sparse as sp\n",
        "from tqdm import tqdm\n",
        "from net import Nettack1 ####### our modified code\n",
        "from nettackonlyadd import NettackAdd  ######for addition only\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from deeprobust.graph import utils\n",
        "import torch.optim as optim\n",
        "from deeprobust.graph.defense import GCN\n",
        "from deeprobust.graph.defense import GAT\n",
        "#from deeprobust.graph.targeted_attack import Nettack\n",
        "from nettack_dpr import Nettack\n",
        "from deeprobust.graph.utils import *\n",
        "from deeprobust.graph.data import Dataset\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "from deeprobust.graph.data import Dataset, Dpr2Pyg, Pyg2Dpr\n",
        "from numba import jit\n",
        "from allfnc import get_linearized_weight, normalize_adj, compute_logits, find_all_2hop_neighbors1,select_lists_with_element1\n",
        "\n",
        "\n",
        "def test1(adj, features, target_node):\n",
        "    ''' test on GCN '''\n",
        "    gcn = GCN(nfeat=features.shape[1], nhid=16, nclass=labels.max().item() + 1, dropout=0.5, device=device)\n",
        "    gcn = gcn.to(device)\n",
        "    gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "    gcn.eval()\n",
        "    output = gcn.predict()\n",
        "    probs = torch.exp(output[[target_node]])[0]\n",
        "    # acc_test = accuracy(output[[target_node]], labels[target_node])\n",
        "    acc_test = (output.argmax(1)[target_node] == labels[target_node])\n",
        "    print('Target node probs: {}'.format(probs.detach().cpu().numpy()))\n",
        "    print(output.argmax(1)[target_node])\n",
        "    print(labels[target_node])\n",
        "    #acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "\n",
        "    #print(\"Overall test set results:\",\n",
        "     #     \"accuracy= {:.4f}\".format(acc_test.item()))\n",
        "\n",
        "    return acc_test.item()\n",
        "\n",
        "\n",
        "\n",
        "data = Dataset(root='C:\\pythonProject1', name='cora')\n",
        "adj, features, labels = data.adj, data.features, data.labels\n",
        "idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "\n",
        "print(data)\n",
        "print(idx_train)\n",
        "\n",
        "idx_unlabeled = np.union1d(idx_val, idx_test)\n",
        "\n",
        "print(idx_unlabeled)\n",
        "print(len(idx_unlabeled))\n",
        "\n",
        "node_list = random.sample(list(idx_unlabeled),10)\n",
        "\n",
        "print(node_list)\n",
        "\n",
        "\n",
        "degrees = adj.sum(0).A1\n",
        "\n",
        "print(degrees)\n",
        "print(degrees[400])"
      ]
    }
  ]
}